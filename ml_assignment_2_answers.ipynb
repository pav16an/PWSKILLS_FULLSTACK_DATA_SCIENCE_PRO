{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pav16an/PWSKILLS_FULLSTACK_DATA_SCIENCE_PRO/blob/main/ml_assignment_2_answers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e9PHe9vmLIW"
      },
      "source": [
        "# Machine Learning Assignment 2 Answers\n",
        "\n",
        "This notebook contains comprehensive answers to the questions from the ml_assignment-2.pdf."
      ],
      "id": "4e9PHe9vmLIW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceIR96UXmLIY"
      },
      "source": [
        "### 1. What is regression analysis?\n",
        "Regression analysis is a statistical method to model and analyze the relationship between a dependent variable and one or more independent variables."
      ],
      "id": "ceIR96UXmLIY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3ZibCFomLIY"
      },
      "source": [
        "### 2. Difference between linear and nonlinear regression:\n",
        "Linear regression models a linear relationship between variables; nonlinear regression models more complex, nonlinear relationships."
      ],
      "id": "O3ZibCFomLIY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6fKPY86mLIZ"
      },
      "source": [
        "### 3. Difference between simple linear regression and multiple linear regression:\n",
        "Simple linear regression uses one independent variable; multiple linear regression uses two or more independent variables."
      ],
      "id": "X6fKPY86mLIZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmJdpSTxmLIZ"
      },
      "source": [
        "### 4. How is the performance of a regression model typically evaluated?\n",
        "Using metrics like Mean Squared Error (MSE), Root Mean Squared Error (RMSE), R-squared."
      ],
      "id": "zmJdpSTxmLIZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ps18kWjmLIZ"
      },
      "source": [
        "### 5. What is overfitting in regression models?\n",
        "When the model fits the training data too closely, capturing noise and failing to generalize."
      ],
      "id": "3ps18kWjmLIZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laLUKjfnmLIa"
      },
      "source": [
        "### 6. What is logistic regression used for?\n",
        "For binary classification problems, predicting probabilities of class membership."
      ],
      "id": "laLUKjfnmLIa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGFvKIRMmLIa"
      },
      "source": [
        "### 7. How does logistic regression differ from linear regression?\n",
        "Logistic regression predicts probabilities using a sigmoid function; linear regression predicts continuous values."
      ],
      "id": "LGFvKIRMmLIa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sH98j6pMmLIa"
      },
      "source": [
        "### 8. Explain the concept of odds ratio in logistic regression:\n",
        "Odds ratio measures the change in odds of the outcome for a one-unit change in the predictor."
      ],
      "id": "sH98j6pMmLIa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYiLojt_mLIa"
      },
      "source": [
        "### 9. What is the sigmoid function in logistic regression?\n",
        "A function that maps any real-valued number into the (0,1) interval, representing probability."
      ],
      "id": "rYiLojt_mLIa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZZ2lTVRmLIb"
      },
      "source": [
        "### 10. How is the performance of a logistic regression model evaluated?\n",
        "Using metrics like accuracy, precision, recall, F1-score, ROC-AUC."
      ],
      "id": "6ZZ2lTVRmLIb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKVFBBFZmLIb"
      },
      "source": [
        "### 11. What is a decision tree?\n",
        "A tree-like model used for classification and regression that splits data based on feature values."
      ],
      "id": "iKVFBBFZmLIb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5cYxSV7mLIb"
      },
      "source": [
        "### 12. How does a decision tree make predictions?\n",
        "By traversing from root to leaf nodes based on feature conditions."
      ],
      "id": "l5cYxSV7mLIb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7e_W8uBmLIb"
      },
      "source": [
        "### 13. What is entropy in decision trees?\n",
        "A measure of impurity or disorder used to decide splits."
      ],
      "id": "N7e_W8uBmLIb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2QakuqgmLIb"
      },
      "source": [
        "### 14. What is pruning in decision trees?\n",
        "Removing parts of the tree to reduce overfitting."
      ],
      "id": "I2QakuqgmLIb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62K_lb4BmLIb"
      },
      "source": [
        "### 15. How do decision trees handle missing values?\n",
        "By surrogate splits or assigning the most common value."
      ],
      "id": "62K_lb4BmLIb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hz8JkfXmLIb"
      },
      "source": [
        "### 16. What is a support vector machine (SVM)?\n",
        "A classifier that finds the hyperplane maximizing the margin between classes."
      ],
      "id": "4hz8JkfXmLIb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysVWaI_jmLIb"
      },
      "source": [
        "### 17. Explain the concept of margin in SVM:\n",
        "The distance between the separating hyperplane and the nearest data points."
      ],
      "id": "ysVWaI_jmLIb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOI4vw2mmLIc"
      },
      "source": [
        "### 18. What are support vectors in SVM?\n",
        "Data points closest to the hyperplane that influence its position."
      ],
      "id": "oOI4vw2mmLIc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJC9DOW0mLIc"
      },
      "source": [
        "### 19. How does SVM handle non-linearly separable data?\n",
        "Using kernel functions to map data into higher dimensions."
      ],
      "id": "IJC9DOW0mLIc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWB2PpiEmLIc"
      },
      "source": [
        "### 20. Advantages of SVM over other classifiers:\n",
        "Effective in high-dimensional spaces, robust to overfitting."
      ],
      "id": "wWB2PpiEmLIc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f8sfCzjmLIc"
      },
      "source": [
        "### 21. What is the Naïve Bayes algorithm?\n",
        "A probabilistic classifier based on Bayes theorem with independence assumptions."
      ],
      "id": "3f8sfCzjmLIc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NPB9jg-mLIc"
      },
      "source": [
        "### 22. Why is it called \"Naïve\" Bayes?\n",
        "Because it assumes feature independence, which is often not true."
      ],
      "id": "_NPB9jg-mLIc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfahsR5FmLIc"
      },
      "source": [
        "### 23. How does Naïve Bayes handle continuous and categorical features?\n",
        "Continuous features are modeled with distributions (e.g., Gaussian); categorical features with frequency counts."
      ],
      "id": "RfahsR5FmLIc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D65DDKzDmLIc"
      },
      "source": [
        "### 24. Explain prior and posterior probabilities in Naïve Bayes:\n",
        "Prior is initial belief about class; posterior is updated belief after seeing data."
      ],
      "id": "D65DDKzDmLIc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2kj_rfLmLId"
      },
      "source": [
        "### 25. What is Laplace smoothing and why is it used?\n",
        "A technique to handle zero probabilities by adding a small value."
      ],
      "id": "K2kj_rfLmLId"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FhJyJEqmLId"
      },
      "source": [
        "### 26. Can Naïve Bayes be used for regression tasks?\n",
        "No, it is primarily for classification."
      ],
      "id": "0FhJyJEqmLId"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCyHeuDmmLId"
      },
      "source": [
        "### 27. How do you handle missing values in Naïve Bayes?\n",
        "By ignoring missing features or imputing values."
      ],
      "id": "VCyHeuDmmLId"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gcc3BmYmLId"
      },
      "source": [
        "### 28. Common applications of Naïve Bayes:\n",
        "Spam filtering, text classification."
      ],
      "id": "2gcc3BmYmLId"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niLrUCLjmLId"
      },
      "source": [
        "### 29. Explain feature independence assumption in Naïve Bayes:\n",
        "Features are assumed to be independent given the class."
      ],
      "id": "niLrUCLjmLId"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8_OkLdDmLId"
      },
      "source": [
        "### 30. How does Naïve Bayes handle categorical features with many categories?\n",
        "By grouping rare categories or using smoothing."
      ],
      "id": "E8_OkLdDmLId"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox-TifSjmLId"
      },
      "source": [
        "### 31. What is the curse of dimensionality?\n",
        "High-dimensional data causes sparsity and degrades model performance."
      ],
      "id": "ox-TifSjmLId"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtXMX9JmmLIe"
      },
      "source": [
        "### 32. Explain bias-variance tradeoff:\n",
        "Balance between underfitting (high bias) and overfitting (high variance)."
      ],
      "id": "LtXMX9JmmLIe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KHEF_nRmLIe"
      },
      "source": [
        "### 33. What is cross-validation and why is it used?\n",
        "A technique to evaluate model generalization by partitioning data."
      ],
      "id": "5KHEF_nRmLIe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_SMs-cwmLIe"
      },
      "source": [
        "### 34. Difference between parametric and non-parametric algorithms:\n",
        "Parametric assume fixed form (e.g., linear regression); non-parametric do not (e.g., KNN)."
      ],
      "id": "r_SMs-cwmLIe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZv7obb4mLIe"
      },
      "source": [
        "### 35. What is feature scaling and why important?\n",
        "Rescaling features to a common scale to improve model performance."
      ],
      "id": "PZv7obb4mLIe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzVYo0tYmLIe"
      },
      "source": [
        "### 36. What is regularization and why used?\n",
        "Technique to prevent overfitting by adding penalty terms."
      ],
      "id": "bzVYo0tYmLIe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEVLjs-UmLIe"
      },
      "source": [
        "### 37. Explain ensemble learning and give example:\n",
        "Combining multiple models to improve performance; e.g., Random Forest."
      ],
      "id": "BEVLjs-UmLIe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMFLwL65mLIe"
      },
      "source": [
        "### 38. Difference between bagging and boosting:\n",
        "Bagging builds models independently; boosting builds sequentially focusing on errors."
      ],
      "id": "KMFLwL65mLIe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euer5zU8mLIk"
      },
      "source": [
        "### 39. Difference between generative and discriminative models:\n",
        "Generative models model joint distribution; discriminative model conditional distribution."
      ],
      "id": "euer5zU8mLIk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhyBddMOmLIk"
      },
      "source": [
        "### 40. Explain batch gradient descent and stochastic gradient descent:\n",
        "Batch uses all data per update; stochastic uses one sample per update."
      ],
      "id": "ZhyBddMOmLIk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLs2x8etmLIl"
      },
      "source": [
        "### 41. What is K-nearest neighbors (KNN) and how it works:\n",
        "Classifies based on majority class among k nearest neighbors."
      ],
      "id": "cLs2x8etmLIl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCmPAZ69mLIm"
      },
      "source": [
        "### 42. Disadvantages of KNN:\n",
        "Computationally expensive, sensitive to irrelevant features."
      ],
      "id": "FCmPAZ69mLIm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kwju2LMCmLIm"
      },
      "source": [
        "### 43. Explain one-hot encoding and its use:\n",
        "Converts categorical variables into binary vectors."
      ],
      "id": "Kwju2LMCmLIm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxmHSGdFmLIm"
      },
      "source": [
        "### 44. What is feature selection and why important:\n",
        "Selecting relevant features to improve model accuracy and reduce complexity."
      ],
      "id": "TxmHSGdFmLIm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmGrvNgymLIm"
      },
      "source": [
        "### 45. Explain cross-entropy loss and its use:\n",
        "Loss function for classification measuring difference between predicted and true labels."
      ],
      "id": "OmGrvNgymLIm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8MngdpemLIm"
      },
      "source": [
        "### 46. Difference between batch learning and online learning:\n",
        "Batch learns from entire dataset; online learns incrementally."
      ],
      "id": "W8MngdpemLIm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uEWBnK8mLIm"
      },
      "source": [
        "### 47. Explain grid search and its use:\n",
        "Systematic hyperparameter tuning method."
      ],
      "id": "8uEWBnK8mLIm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMvA8S5XmLIm"
      },
      "source": [
        "### 48. Advantages and disadvantages of decision trees:\n",
        "Easy to interpret but prone to overfitting."
      ],
      "id": "VMvA8S5XmLIm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdtIhBjzmLIm"
      },
      "source": [
        "### 49. Difference between L1 and L2 regularization:\n",
        "L1 promotes sparsity; L2 penalizes large weights."
      ],
      "id": "YdtIhBjzmLIm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VMfCvTnmLIm"
      },
      "source": [
        "### 50. Common preprocessing techniques:\n",
        "Normalization, encoding, missing value imputation."
      ],
      "id": "9VMfCvTnmLIm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2caBBy1CmLIm"
      },
      "source": [
        "### 51. Difference between parametric and non-parametric algorithms with examples:\n",
        "Parametric: Linear regression; Non-parametric: KNN."
      ],
      "id": "2caBBy1CmLIm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfRjyFX3mLIn"
      },
      "source": [
        "### 52. Bias-variance tradeoff and model complexity:\n",
        "Complex models have low bias, high variance; simple models vice versa."
      ],
      "id": "FfRjyFX3mLIn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ij76Fw_QmLIn"
      },
      "source": [
        "### 53. Advantages and disadvantages of ensemble methods like random forests:\n",
        "Improved accuracy but less interpretable."
      ],
      "id": "Ij76Fw_QmLIn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VMb7DwwmLIn"
      },
      "source": [
        "### 54. Difference between bagging and boosting:\n",
        "Bagging reduces variance; boosting reduces bias."
      ],
      "id": "5VMb7DwwmLIn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9fP0PZPmLIn"
      },
      "source": [
        "### 55. Purpose of hyperparameter tuning:\n",
        "Optimize model performance."
      ],
      "id": "k9fP0PZPmLIn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsGd0ea-mLIn"
      },
      "source": [
        "### 56. Difference between regularization and feature selection:\n",
        "Regularization penalizes complexity; feature selection removes features."
      ],
      "id": "nsGd0ea-mLIn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Js88RX8fmLIn"
      },
      "source": [
        "### 57. How Lasso (L1) differs from Ridge (L2) regularization:\n",
        "Lasso can zero out coefficients; Ridge shrinks coefficients."
      ],
      "id": "Js88RX8fmLIn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNtAbXlRmLIn"
      },
      "source": [
        "### 58. Explain cross-validation and why used:\n",
        "Repeated model evaluation to ensure generalization."
      ],
      "id": "pNtAbXlRmLIn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpQ4011wmLIn"
      },
      "source": [
        "### 59. Common evaluation metrics for regression:\n",
        "MSE, RMSE, MAE, R-squared."
      ],
      "id": "wpQ4011wmLIn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHiNyQj4mLIo"
      },
      "source": [
        "### 60. How KNN makes predictions:\n",
        "By majority vote of nearest neighbors."
      ],
      "id": "ZHiNyQj4mLIo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-nNtnNhmLIo"
      },
      "source": [
        "### 61. Curse of dimensionality and its effect:\n",
        "High dimensions cause data sparsity, reducing model effectiveness."
      ],
      "id": "S-nNtnNhmLIo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-31N1ZXmLIo"
      },
      "source": [
        "### 62. Importance of feature scaling:\n",
        "Ensures features contribute equally."
      ],
      "id": "2-31N1ZXmLIo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7t0YJ4YDmLIo"
      },
      "source": [
        "### 63. How Naïve Bayes handles categorical features:\n",
        "Uses frequency counts and smoothing."
      ],
      "id": "7t0YJ4YDmLIo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnEYPyffmLIo"
      },
      "source": [
        "### 64. Explain prior and posterior probabilities in Naïve Bayes:\n",
        "Prior is initial class probability; posterior updated after data."
      ],
      "id": "hnEYPyffmLIo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtE5n366mLIo"
      },
      "source": [
        "### 65. What is Laplace smoothing and why used:\n",
        "Avoid zero probabilities in categorical data."
      ],
      "id": "RtE5n366mLIo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpqhmeTtmLIp"
      },
      "source": [
        "### 66. Can Naïve Bayes handle continuous features:\n",
        "Yes, using Gaussian distribution."
      ],
      "id": "gpqhmeTtmLIp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7agCAzvmLIp"
      },
      "source": [
        "### 67. Assumptions of Naïve Bayes:\n",
        "Feature independence."
      ],
      "id": "Q7agCAzvmLIp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxNflfdemLIp"
      },
      "source": [
        "### 68. How Naïve Bayes handles missing values:\n",
        "Ignores or imputes missing features."
      ],
      "id": "BxNflfdemLIp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gZaFsZ8mLIp"
      },
      "source": [
        "### 69. Common applications of Naïve Bayes:\n",
        "Text classification, spam detection."
      ],
      "id": "1gZaFsZ8mLIp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJadxUgYmLIp"
      },
      "source": [
        "### 70. Difference between generative and discriminative models:\n",
        "Generative models model data distribution; discriminative models model decision boundary."
      ],
      "id": "eJadxUgYmLIp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv7spK-ImLIq"
      },
      "source": [
        "### 71. Decision boundary of Naïve Bayes for binary classification:\n",
        "Typically linear or quadratic depending on distribution."
      ],
      "id": "mv7spK-ImLIq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgoEUlpPmLIq"
      },
      "source": [
        "### 72. Difference between multinomial and Gaussian Naïve Bayes:\n",
        "Multinomial for discrete counts; Gaussian for continuous data."
      ],
      "id": "wgoEUlpPmLIq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY8biugTmLIq"
      },
      "source": [
        "### 73. How Naïve Bayes handles numerical instability:\n",
        "Using log probabilities."
      ],
      "id": "BY8biugTmLIq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzxuGFPxmLIq"
      },
      "source": [
        "### 74. What is Laplacian correction and when used:\n",
        "Same as Laplace smoothing, used to handle zero counts."
      ],
      "id": "bzxuGFPxmLIq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jM7xuKGHmLIq"
      },
      "source": [
        "### 75. Can Naïve Bayes be used for regression:\n",
        "No."
      ],
      "id": "jM7xuKGHmLIq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69F_onKjmLIq"
      },
      "source": [
        "### 76. Explain conditional independence assumption in Naïve Bayes:\n",
        "Features are independent given the class."
      ],
      "id": "69F_onKjmLIq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDVmFXP3mLIq"
      },
      "source": [
        "### 77. How Naïve Bayes handles categorical features with many categories:\n",
        "Grouping or smoothing."
      ],
      "id": "UDVmFXP3mLIq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vR8WAMZamLIq"
      },
      "source": [
        "### 78. Drawbacks of Naïve Bayes:\n",
        "Strong independence assumption, poor with correlated features."
      ],
      "id": "vR8WAMZamLIq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eb-BEMbbmLIr"
      },
      "source": [
        "### 79. Explain smoothing in Naïve Bayes:\n",
        "Technique to handle zero probabilities."
      ],
      "id": "Eb-BEMbbmLIr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAN-NuO8mLIr"
      },
      "source": [
        "### 80. How Naïve Bayes handles imbalanced datasets:\n",
        "Using class priors or resampling."
      ],
      "id": "VAN-NuO8mLIr"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
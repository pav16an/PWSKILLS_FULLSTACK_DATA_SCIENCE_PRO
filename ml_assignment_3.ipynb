{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pav16an/PWSKILLS_FULLSTACK_DATA_SCIENCE_PRO/blob/main/ml_assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gBsJsJ_mY1-"
      },
      "source": [
        "# Machine Learning Assignment 3 Answers\n",
        "\n",
        "This notebook contains comprehensive answers to the questions from the file 6650744c3afdd313879dc8b5.pdf."
      ],
      "id": "1gBsJsJ_mY1-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHQTUjLHmY1_"
      },
      "source": [
        "### 1. What are ensemble techniques in machine learning?\n",
        "Ensemble techniques combine multiple models to improve predictive performance by reducing variance, bias, or improving predictions."
      ],
      "id": "hHQTUjLHmY1_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_SStf_GmY1_"
      },
      "source": [
        "### 2. Explain bagging and how it works in ensemble techniques:\n",
        "Bagging (Bootstrap Aggregating) trains multiple models on different bootstrapped samples of the data and aggregates their predictions, typically by voting or averaging."
      ],
      "id": "1_SStf_GmY1_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hUmuwbemY2A"
      },
      "source": [
        "### 3. What is the purpose of bootstrapping in bagging?\n",
        "Bootstrapping creates diverse training sets by sampling with replacement, which helps reduce variance."
      ],
      "id": "0hUmuwbemY2A"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_cWKF9WmY2A"
      },
      "source": [
        "### 4. Describe the random forest algorithm:\n",
        "Random forest builds multiple decision trees using bootstrapped data and random subsets of features, then aggregates their predictions."
      ],
      "id": "n_cWKF9WmY2A"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2fKNtfymY2A"
      },
      "source": [
        "### 5. How does randomization reduce overfitting in random forests?\n",
        "Random feature selection and bootstrapping reduce correlation among trees, lowering overfitting."
      ],
      "id": "w2fKNtfymY2A"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZehEK0qmY2A"
      },
      "source": [
        "### 6. Explain the concept of feature bagging in random forests:\n",
        "Feature bagging randomly selects subsets of features for each tree split to increase diversity."
      ],
      "id": "NZehEK0qmY2A"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ds0TbcqVmY2A"
      },
      "source": [
        "### 7. What is the role of decision trees in gradient boosting?\n",
        "Decision trees are weak learners sequentially trained to correct errors of previous trees."
      ],
      "id": "ds0TbcqVmY2A"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzphe5rmmY2B"
      },
      "source": [
        "### 8. Differentiate between bagging and boosting:\n",
        "Bagging builds models independently; boosting builds models sequentially focusing on errors."
      ],
      "id": "dzphe5rmmY2B"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5U1bLDmmY2B"
      },
      "source": [
        "### 9. What is the AdaBoost algorithm, and how does it work?\n",
        "AdaBoost assigns weights to samples, trains weak learners sequentially, and adjusts weights to focus on misclassified samples."
      ],
      "id": "U5U1bLDmmY2B"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32trl-GumY2B"
      },
      "source": [
        "### 10. Explain the concept of weak learners in boosting algorithms:\n",
        "Weak learners are models slightly better than random guessing, combined to form a strong learner."
      ],
      "id": "32trl-GumY2B"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3AKhMNWmY2B"
      },
      "source": [
        "### 11. Describe the process of adaptive boosting:\n",
        "Sequentially trains weak learners, adjusting sample weights to emphasize difficult cases."
      ],
      "id": "y3AKhMNWmY2B"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zf5LCK1-mY2B"
      },
      "source": [
        "### 12. How does AdaBoost adjust weights for misclassified data points?\n",
        "Increases weights of misclassified points to focus learning on them."
      ],
      "id": "Zf5LCK1-mY2B"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjpAriXcmY2C"
      },
      "source": [
        "### 13. Discuss the XGBoost algorithm and its advantages over traditional gradient boosting:\n",
        "XGBoost is an optimized gradient boosting implementation with regularization, parallel processing, and handling missing data."
      ],
      "id": "mjpAriXcmY2C"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwShjXbMmY2C"
      },
      "source": [
        "### 14. Explain the concept of regularization in XGBoost:\n",
        "Regularization penalizes model complexity to prevent overfitting."
      ],
      "id": "gwShjXbMmY2C"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJt3Fb_cmY2C"
      },
      "source": [
        "### 15. What are the different types of ensemble techniques?\n",
        "Bagging, boosting, stacking."
      ],
      "id": "QJt3Fb_cmY2C"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNF-R_VSmY2C"
      },
      "source": [
        "### 16. Compare and contrast bagging and boosting:\n",
        "Bagging reduces variance by averaging; boosting reduces bias by sequential correction."
      ],
      "id": "FNF-R_VSmY2C"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWlPMX5JmY2C"
      },
      "source": [
        "### 17. Discuss the concept of ensemble diversity:\n",
        "Diverse models reduce correlated errors, improving ensemble performance."
      ],
      "id": "XWlPMX5JmY2C"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVnn965cmY2C"
      },
      "source": [
        "### 18. How do ensemble techniques improve predictive performance?\n",
        "By combining multiple models, they reduce errors and improve generalization."
      ],
      "id": "JVnn965cmY2C"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sy4LlkEymY2C"
      },
      "source": [
        "### 19. Explain the concept of ensemble variance and bias:\n",
        "Variance is model sensitivity to data fluctuations; bias is error from incorrect assumptions."
      ],
      "id": "sy4LlkEymY2C"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCK8BvoAmY2D"
      },
      "source": [
        "### 20. Discuss the trade-off between bias and variance in ensemble learning:\n",
        "Ensembles aim to reduce both bias and variance for better accuracy."
      ],
      "id": "fCK8BvoAmY2D"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqYXFZQgmY2D"
      },
      "source": [
        "### 21. What are some common applications of ensemble techniques?\n",
        "Fraud detection, image classification, recommendation systems."
      ],
      "id": "bqYXFZQgmY2D"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2Whc0qXmY2D"
      },
      "source": [
        "### 22. How does ensemble learning contribute to model interpretability?\n",
        "Some ensembles like random forests provide feature importance; others are less interpretable."
      ],
      "id": "Q2Whc0qXmY2D"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCKgy_ZemY2D"
      },
      "source": [
        "### 23. Describe the process of stacking in ensemble learning:\n",
        "Combines predictions of base models using a meta-learner."
      ],
      "id": "mCKgy_ZemY2D"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUSZ2Az_mY2D"
      },
      "source": [
        "### 24. Discuss the role of meta-learners in stacking:\n",
        "Meta-learners learn to optimally combine base model outputs."
      ],
      "id": "GUSZ2Az_mY2D"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeYG8Da1mY2D"
      },
      "source": [
        "### 25. What are some challenges associated with ensemble techniques?\n",
        "Increased complexity, computational cost, reduced interpretability."
      ],
      "id": "HeYG8Da1mY2D"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvVVCq_YmY2D"
      },
      "source": [
        "### 26. What is boosting, and how does it differ from bagging?\n",
        "Boosting builds models sequentially focusing on errors; bagging builds independently."
      ],
      "id": "nvVVCq_YmY2D"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyv15dXvmY2D"
      },
      "source": [
        "### 27. Explain the intuition behind boosting:\n",
        "Focus learning on hard-to-predict samples to improve accuracy."
      ],
      "id": "xyv15dXvmY2D"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXh5ytrjmY2D"
      },
      "source": [
        "### 28. Describe the concept of sequential training in boosting:\n",
        "Models are trained one after another, each correcting previous errors."
      ],
      "id": "vXh5ytrjmY2D"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hcU91GHmY2D"
      },
      "source": [
        "### 29. How does boosting handle misclassified data points?\n",
        "By increasing their weights for subsequent learners."
      ],
      "id": "3hcU91GHmY2D"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXaMeioFmY2D"
      },
      "source": [
        "### 30. Discuss the role of weights in boosting algorithms:\n",
        "Weights emphasize difficult samples to guide learning."
      ],
      "id": "uXaMeioFmY2D"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHZbzMYmmY2D"
      },
      "source": [
        "### 31. What is the difference between boosting and AdaBoost?\n",
        "AdaBoost is a specific boosting algorithm with adaptive weighting."
      ],
      "id": "OHZbzMYmmY2D"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw0J5o8CmY2E"
      },
      "source": [
        "### 32. How does AdaBoost adjust weights for misclassified samples?\n",
        "Increases weights exponentially for misclassified points."
      ],
      "id": "aw0J5o8CmY2E"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZQudANHmY2E"
      },
      "source": [
        "### 33. Explain the concept of weak learners in boosting algorithms:\n",
        "Models with performance slightly better than random guessing."
      ],
      "id": "1ZQudANHmY2E"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNa4WZiMmY2E"
      },
      "source": [
        "### 34. Discuss the process of gradient boosting:\n",
        "Sequentially fits models to residual errors using gradient descent."
      ],
      "id": "iNa4WZiMmY2E"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IdoEMxfmY2E"
      },
      "source": [
        "### 35. What is the purpose of gradient descent in gradient boosting?\n",
        "To minimize loss by updating model parameters."
      ],
      "id": "8IdoEMxfmY2E"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snQhw-DWmY2E"
      },
      "source": [
        "### 36. Describe the role of learning rate in gradient boosting:\n",
        "Controls contribution of each tree to prevent overfitting."
      ],
      "id": "snQhw-DWmY2E"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYdQdtFqmY2E"
      },
      "source": [
        "### 37. How does gradient boosting handle overfitting?\n",
        "Using regularization, learning rate, and early stopping."
      ],
      "id": "fYdQdtFqmY2E"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIhS-voEmY2E"
      },
      "source": [
        "### 38. Discuss the differences between gradient boosting and XGBoost:\n",
        "XGBoost adds regularization, parallelism, and system optimizations."
      ],
      "id": "yIhS-voEmY2E"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aT6lHr4ImY2E"
      },
      "source": [
        "### 39. Explain the concept of regularized boosting:\n",
        "Boosting with penalties to reduce model complexity."
      ],
      "id": "aT6lHr4ImY2E"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75PeC0BkmY2F"
      },
      "source": [
        "### 40. What are the advantages of using XGBoost over traditional gradient boosting?\n",
        "Faster training, better accuracy, handling missing data."
      ],
      "id": "75PeC0BkmY2F"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVfTNuxemY2F"
      },
      "source": [
        "### 41. Describe the process of early stopping in boosting algorithms:\n",
        "Stops training when validation error stops improving."
      ],
      "id": "xVfTNuxemY2F"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odBXNeUAmY2F"
      },
      "source": [
        "### 42. How does early stopping prevent overfitting?\n",
        "Prevents unnecessary training beyond optimal point."
      ],
      "id": "odBXNeUAmY2F"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBeOWbNomY2F"
      },
      "source": [
        "### 43. Discuss the role of hyperparameters in boosting algorithms:\n",
        "Control model complexity and learning behavior."
      ],
      "id": "PBeOWbNomY2F"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8NATN4SmY2F"
      },
      "source": [
        "### 44. What are some common challenges associated with boosting?\n",
        "Overfitting, sensitivity to noisy data."
      ],
      "id": "E8NATN4SmY2F"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6OeeLJNmY2F"
      },
      "source": [
        "### 45. Explain the concept of boosting convergence:\n",
        "Model performance stabilizes as more learners are added."
      ],
      "id": "k6OeeLJNmY2F"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGlQouJpmY2G"
      },
      "source": [
        "### 46. How does boosting improve the performance of weak learners?\n",
        "By combining them to form a strong learner."
      ],
      "id": "WGlQouJpmY2G"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Z7q2lJDmY2G"
      },
      "source": [
        "### 47. Discuss the impact of data imbalance on boosting algorithms:\n",
        "May bias towards majority class; requires handling."
      ],
      "id": "7Z7q2lJDmY2G"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3iydkINmY2G"
      },
      "source": [
        "### 48. What are some real-world applications of boosting?\n",
        "Credit scoring, customer churn prediction."
      ],
      "id": "j3iydkINmY2G"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nsvowc2pmY2G"
      },
      "source": [
        "### 49. Describe the process of ensemble selection in boosting:\n",
        "Choosing best subset of models for final prediction."
      ],
      "id": "Nsvowc2pmY2G"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYNJGyGkmY2G"
      },
      "source": [
        "### 50. How does boosting contribute to model interpretability?\n",
        "Provides feature importance but less transparent than single models."
      ],
      "id": "hYNJGyGkmY2G"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePIAB98TmY2G"
      },
      "source": [
        "### 51. Explain the curse of dimensionality and its impact on KNN:\n",
        "High dimensions cause distance measures to lose meaning, degrading KNN."
      ],
      "id": "ePIAB98TmY2G"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6OSqGsimY2G"
      },
      "source": [
        "### 52. What are the applications of KNN in real-world scenarios?\n",
        "Recommendation systems, image recognition."
      ],
      "id": "C6OSqGsimY2G"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kquG_2oamY2G"
      },
      "source": [
        "### 53. Discuss the concept of weighted KNN:\n",
        "Weights neighbors by distance for prediction."
      ],
      "id": "kquG_2oamY2G"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfEdfy5HmY2G"
      },
      "source": [
        "### 54. How do you handle missing values in KNN?\n",
        "Impute missing data or use distance metrics that handle missingness."
      ],
      "id": "EfEdfy5HmY2G"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tBCkfrUmY2H"
      },
      "source": [
        "### 55. Explain the difference between lazy learning and eager learning algorithms, and where does KNN fit in:\n",
        "KNN is lazy learning; it delays generalization until prediction."
      ],
      "id": "2tBCkfrUmY2H"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZr7vcvHmY2H"
      },
      "source": [
        "### 56. What are some methods to improve the performance of KNN?\n",
        "Feature scaling, dimensionality reduction."
      ],
      "id": "vZr7vcvHmY2H"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkEl1EgMmY2H"
      },
      "source": [
        "### 57. Can KNN be used for regression tasks? If yes, how?\n",
        "Yes, by averaging target values of neighbors."
      ],
      "id": "IkEl1EgMmY2H"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOpUDGUHmY2H"
      },
      "source": [
        "### 58. Describe the boundary decision made by the KNN algorithm:\n",
        "Decision boundary is non-linear and depends on neighbors."
      ],
      "id": "pOpUDGUHmY2H"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvwOT25NmY2H"
      },
      "source": [
        "### 59. How do you choose the optimal value of K in KNN?\n",
        "Using cross-validation or elbow method."
      ],
      "id": "EvwOT25NmY2H"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWfjhzRkmY2H"
      },
      "source": [
        "### 60. Discuss the trade-offs between using a small and large value of K in KNN:\n",
        "Small K is sensitive to noise; large K smooths but may miss patterns."
      ],
      "id": "KWfjhzRkmY2H"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iTkOPaYmY2H"
      },
      "source": [
        "### 61. Explain the process of feature scaling in the context of KNN:\n",
        "Rescale features to prevent dominance by large-scale features."
      ],
      "id": "_iTkOPaYmY2H"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsI1e0slmY2H"
      },
      "source": [
        "### 62. Compare and contrast KNN with other classification algorithms like SVM and Decision Trees:\n",
        "KNN is instance-based and non-parametric; SVM finds optimal hyperplane; decision trees split data hierarchically."
      ],
      "id": "gsI1e0slmY2H"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CH17PQOVmY2H"
      },
      "source": [
        "### 63. How does the choice of distance metric affect the performance of KNN:\n",
        "Different metrics capture different notions of similarity."
      ],
      "id": "CH17PQOVmY2H"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1C9ALSnmY2H"
      },
      "source": [
        "### 64. What are some techniques to deal with imbalanced datasets in KNN:\n",
        "Resampling, weighted voting."
      ],
      "id": "q1C9ALSnmY2H"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4s_1FQemY2H"
      },
      "source": [
        "### 65. Explain the concept of cross-validation in the context of tuning KNN parameters:\n",
        "Partition data to evaluate model stability."
      ],
      "id": "x4s_1FQemY2H"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J826NpbdmY2H"
      },
      "source": [
        "### 66. What is the difference between uniform and distance-weighted voting in KNN:\n",
        "Uniform treats all neighbors equally; distance-weighted gives closer neighbors more influence."
      ],
      "id": "J826NpbdmY2H"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utupgGPOmY2I"
      },
      "source": [
        "### 67. Discuss the computational complexity of KNN:\n",
        "High at prediction time due to distance calculations."
      ],
      "id": "utupgGPOmY2I"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpHpCyq_mY2I"
      },
      "source": [
        "### 68. How does the choice of distance metric impact the sensitivity of KNN to outliers:\n",
        "Some metrics are more robust to outliers."
      ],
      "id": "DpHpCyq_mY2I"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzBJP2tKmY2I"
      },
      "source": [
        "### 69. Explain the process of selecting an appropriate value for K using the elbow method:\n",
        "Plot error vs K and choose K at the elbow point."
      ],
      "id": "dzBJP2tKmY2I"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IcXsjMXmY2I"
      },
      "source": [
        "### 70. Can KNN be used for text classification tasks? If yes, how:\n",
        "Yes, using vectorized text features."
      ],
      "id": "5IcXsjMXmY2I"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BisnAlmomY2I"
      },
      "source": [
        "### 71. How do you decide the number of principal components to retain in PCA:\n",
        "Using explained variance threshold."
      ],
      "id": "BisnAlmomY2I"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg_Zb0mqmY2I"
      },
      "source": [
        "### 72. Explain the reconstruction error in the context of PCA:\n",
        "Difference between original and reconstructed data."
      ],
      "id": "cg_Zb0mqmY2I"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu748b_hmY2I"
      },
      "source": [
        "### 73. What are the applications of PCA in real-world scenarios:\n",
        "Image compression, noise reduction."
      ],
      "id": "Iu748b_hmY2I"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p335bsLemY2I"
      },
      "source": [
        "### 74. Discuss the limitations of PCA:\n",
        "Linear method, sensitive to outliers."
      ],
      "id": "p335bsLemY2I"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLtHCr6LmY2I"
      },
      "source": [
        "### 75. What is Singular Value Decomposition (SVD), and how is it related to PCA:\n",
        "SVD is a matrix factorization used to compute PCA."
      ],
      "id": "aLtHCr6LmY2I"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTUFn7BqmY2I"
      },
      "source": [
        "### 76. Explain the concept of latent semantic analysis (LSA) and its application in natural language processing:\n",
        "LSA uses SVD to extract topics from text."
      ],
      "id": "rTUFn7BqmY2I"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKAioXMNmY2I"
      },
      "source": [
        "### 77. What are some alternatives to PCA for dimensionality reduction:\n",
        "t-SNE, ICA, autoencoders."
      ],
      "id": "sKAioXMNmY2I"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ydOBxUpmY2I"
      },
      "source": [
        "### 78. Describe t-distributed Stochastic Neighbor Embedding (t-SNE) and its advantages over PCA:\n",
        "Non-linear method preserving local structure."
      ],
      "id": "4ydOBxUpmY2I"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ixpZ7gNmY2J"
      },
      "source": [
        "### 79. How does t-SNE preserve local structure compared to PCA:\n",
        "By modeling pairwise similarities."
      ],
      "id": "_ixpZ7gNmY2J"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5vAF0aTmY2J"
      },
      "source": [
        "### 80. Discuss the limitations of t-SNE:\n",
        "Computationally expensive, non-parametric."
      ],
      "id": "F5vAF0aTmY2J"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpL9j_UrmY2J"
      },
      "source": [
        "### 81. What is the difference between PCA and Independent Component Analysis (ICA):\n",
        "PCA finds uncorrelated components; ICA finds statistically independent components."
      ],
      "id": "TpL9j_UrmY2J"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLHSH-M9mY2J"
      },
      "source": [
        "### 82. Explain the concept of manifold learning and its significance in dimensionality reduction:\n",
        "Non-linear dimensionality reduction preserving data geometry."
      ],
      "id": "wLHSH-M9mY2J"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caDrnkJSmY2J"
      },
      "source": [
        "### 83. What are autoencoders, and how are they used for dimensionality reduction:\n",
        "Neural networks that learn compressed representations."
      ],
      "id": "caDrnkJSmY2J"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwsxPkYgmY2J"
      },
      "source": [
        "### 84. Discuss the challenges of using nonlinear dimensionality reduction techniques:\n",
        "Computational cost, interpretability."
      ],
      "id": "BwsxPkYgmY2J"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Wdx1FC4mY2J"
      },
      "source": [
        "### 85. How does the choice of distance metric impact the performance of dimensionality reduction techniques:\n",
        "Affects neighborhood preservation."
      ],
      "id": "-Wdx1FC4mY2J"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xyw50F7wmY2J"
      },
      "source": [
        "### 86. What are some techniques to visualize high-dimensional data after dimensionality reduction:\n",
        "Scatter plots, heatmaps."
      ],
      "id": "Xyw50F7wmY2J"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPa_QDDcmY2J"
      },
      "source": [
        "### 87. Explain the concept of feature hashing and its role in dimensionality reduction:\n",
        "Hashing features to reduce dimensionality efficiently."
      ],
      "id": "nPa_QDDcmY2J"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orHh_2p2mY2J"
      },
      "source": [
        "### 88. What is the difference between global and local feature extraction methods:\n",
        "Global methods consider entire data; local focus on neighborhoods."
      ],
      "id": "orHh_2p2mY2J"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFKzn1hEmY2J"
      },
      "source": [
        "### 89. How does feature sparsity affect the performance of dimensionality reduction techniques:\n",
        "Sparse data can degrade performance."
      ],
      "id": "qFKzn1hEmY2J"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlpsRnzAmY2K"
      },
      "source": [
        "### 90. Discuss the impact of outliers on dimensionality reduction algorithms:\n",
        "Outliers can distort components and embeddings."
      ],
      "id": "FlpsRnzAmY2K"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}